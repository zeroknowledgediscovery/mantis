\documentclass[9pt,journal]{IEEEtran}
\usepackage{preamble}
\input{custom_defs.tex}
%\usepackage[switch]{lineno}
\draftQtrue
\tikzXtrue
% \tikzXfalse  


\def\highlight{RoyalBlue}
%\alph


% #########################################
% #########################################
% #########################################
% #########################################
\begin{document}
%\linenumbers
\title{\TITLE}
\author{\authora and~\authorc$^\star$\\University of Chicago, Chicago, IL
  % <-this % stops a space
  \IEEEcompsocitemizethanks{\IEEEcompsocthanksitem 
    % note need leading \protect in front of \\ to get a newline within \thanks as
    % \\ is fragile and will error, could use \hfil\break instead.
    $^\star$Corresponding Author E-mail: ishanu@uchicago.edu
    % \IEEEcompsocthanksitem Y. Huang  and V. Rotaru  are with The Unievrsity of Chicago.
  }
  %
}
% \IEEEtitleabstractindextext{%
%   \begin{abstract}
%     % Comparing and contrasting subtle historical patterns is central to  time series analysis. Here we introduce a new approach to quantify deviations in the underlying hidden stochastic generators of sequential discrete-valued data streams.     The proposed measure is universal in the sense that we can compare  data streams     without any feature engineering step, and without the need of any hyper-parameters.     Our core idea here is the generalization of the Kullback-Leibler (KL) divergence, often used to compare probability distributions, to a notion of divergence between finite-valued ergodic stationary stochastic processes. Using this notion of process divergence, we craft a measure of deviation on finite sample paths which we call the sequence likelihood divergence (\ALGONAME) which approximates  a metric on the space of the underlying generators within a well-defined class of discrete-valued stochastic processes.
    
%   \end{abstract}
% }
 \maketitle 
% \IEEEdisplaynontitleabstractindextext

\allowdisplaybreaks{

  \section{Motivation: What is an anomaly?}
Anomalies are outliers,  exceptions, or  aberrations in data that do not conform to some well defined notion of normal
behavior~\cite{chandola2009anomaly}. Exhaustive enumeration of normal behaviors, to  then isolate deviations as  anomalous patterns, is generally ineffective on account of the following:
\begin{enumerate}
\item Enumerating all normal behaviors is typically infeasible. In addition, normality is often application dependent, requiring extensive subject matter expertise for characterization.  Enumerating  anomalous behaviors is even more problematic.
\item Normal behaviors  often evolve,  and a current notion of normal  might not be sufficiently representative in future.
\item Labeled data to train anomaly classifiers is often absent.
\item Measurement noise might confound anomalies, or make normal behavior seem anomalous.
\item In adversarial scenarios, anomalies might be intentionally crafted to manifest normal characteristics.
\end{enumerate}
Anomaly detection algorithms have investigated diverse strategies to address these concerns to varying degrees of success~\cite{markou2003novelty,hodge2004survey,chandola2009anomaly}.
  % In an Isolation Forest, randomly sub-sampled data is processed in a tree structure based on randomly selected features. The samples that travel deeper into the tree are less likely to be anomalies as they required more cuts to isolate them. Similarly, the samples which end up in shorter branches indicate anomalies as it was easier for the tree to separate them from other observations.

\subsection{Anomaly Discovery in Dynamical Systems}
  %nomaly detection is historically well-researched~\cite{markou2003novelty,hodge2004survey,chandola2009anomaly}, where diverse use-cases and application domains have been investigated. %Commonly, however, pattern templates are known a priori that specify expected or normal behaviors. Often, patterns that constitute deviations from  normal behaviors are also known a priori,  with the problem then reducing to \textit{detecting} when any of these known  anomalous behaviors arise in the system of interest. In contrast,
  In this study,  we investigate \textit{anomaly discovery} in dynamical systems, where  templates of normal or anomalous dynamical behaviors are not known a priori. Thus, 1) we necessarily do not know a priori  how many of acceptable or normal behaviors exist or what they look like, or if they evolve with time, and 2) we also do not have any prior knowledge of what constitutes an anomaly  in the system of interest. In such a scenario, conceptualizing a well-founded notion of an anomaly is itself non-trivial.

  Here, we consider sequential data streams as our observations, which are generated by a  dynamical system whose rules of evolution or operation are not directly observable. Consistent with its intuitive definition, we conceptualize an anomaly as a dynamical pattern emerging within   a observation sequence that is \textit{uncommon, unexpected, and/or is poorly explained as having simply arisen by chance}.
  Thus, in our framework, anomalies  arise due to a non-random  deviation in the   underlying dynamical system structure or its parameters. Allowing for the possibility that the system of interest might switch between different dynamical regimes, we intend to discover these patterns of switching, and then characterize when an anomalous switch transpires. In such a setting, the problem is  complicated by the need to recognize distinct regimes,  back out the models of dynamical operation in these regimes, and detect when an unexpected switch occurs; one that is not explainable as a likely noise artifact.

  \subsection{Relation to Anomaly Discovery in Tabular Data}
  In tabular data $i.e.$ where we have samples specified by feature-values, the isolation forest~\cite{liu2008isolation} is an effective approach to unsupervised anomaly detection, which directly characterizes anomalies  without first profiling normal behavior. The isolation forest  identifies anomalies via binary trees, exploiting the fact that in a recursive partitioning of the feature space, anomalies on account of their assumed rarity, will typically be isolated using smaller number of partitions compared to normal samples. The algorithm is  time and memory efficient, and is thus applicable  in high dimensional problems that have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies.

However,   the isolation forest algorithm does not automatically transfer to a dynamical system setting, $e.g$, where our objects of interest are not samples specified by features, but are time series of possibly unequal observation lengths. The notion of an anomaly is this setting is also more intrinsic to system dynamics; while we also assume that anomalies are rare, the key defining feature here  is a non-random deviation in the underlying dynamical rules, which are generally not directly observable, or known a priori. 

\subsection{Key Insight for Patternly}
The key insight in the patternly approach is that dynamical behavior can be modeled efficiently with stochastic generators represented as a special class of probabilistic finite state automata (PFSA)~\cite{chattopadhyay2008structural,chattopadhyay2013abductive}. Representation of dynamics via a PFSA is applicable to both data streams that have a finite alphabet, or continuous-values, with the latter case requiring quantization of the values over a finite alphabet. Importantly, the special class of models that our PFSA framework refers to has efficient inference algorithms, and other important mathematical properies that suggests that a large class fo ergodic stationary finite-valued processes can be exactly represnted in this framework~\cite{chattopadhyay2020deep}. While for continuous-valued data astreams, quantization invariably loses information, discovering anomalies via deviation of  models from adequately quantized data is still a viable approach: unexpected deviations in the quantized representaion is sufficient but not necessary to detect an emergent anomaly in the original data.

  \section{Background \& Prior Work}
  \label{sec:Background}

To obtain a general solution to the anomaly discovery problem in dynamical systems, we need a modeling framework for observational data streams, that infers a model of the underlying dynamical system. We carry out this inference within the framework of probabilistic finite automata, which have been shown to be expressive enough to model a large class of ergodic stationary stochastic processes, evolving over a finite alphabet of values~\cite{chattopadhyay2020deep,chattopadhyay2014data,crutchfield1994calculi,chattopadhyay2008structural}, while admitting efficient inference algorithms~\cite{chattopadhyay2013abductive}.
  
 \subsection{Probabilistic Finite State Automata}
   \begin{defn}[PFSA]
     \label{defn:PFSA}
     A probabilistic finite-state automaton $G$ is a quadruple $\paren{Q, \Sigma, \delta, \pitilde}$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet, $\delta:Q\times\Sigma \rightarrow Q$ called transition map, and $\pitilde:Q \times \Sigma \rightarrow [0,1]$ specifies observation probabilities, with $\forall q \in Q, \sum_{\sigma \in \Sigma} \pitilde(q,\sigma) = 1$.  
   \end{defn}
   We use lower case Greeks (e.g.~$\sigma$ or $\tau$) for symbols in $\Sigma$ and lower case Latins (e.g.~$x$ or $y$) to denote sequence of symbols, with the empty sequence denoted by $\lambda$. 
   The length of a sequence $x$ is denoted by $|x|$. 
   The set of sequences of length $d$ is denoted by $\Sigma^{d}$.
 The directed graph (with possible loops) with vertices in $Q$ and edges specified by $\delta$ is called the graph of the PFSA and, unless stated otherwise, assumed to be strongly connected~\cite{bondy2008graph}.
   \begin{defn}[Observation and Transition Matrices]
     Given a PFSA $\paren{Q, \Sigma, \delta, \pitilde}$, the observation matrix $\Pitilde_{G}$ is the $\abs{Q}\times\abs{\Sigma}$ matrix with the $(q,\sigma)$-entry given by $\pitilde(q, \sigma)$, and the transition matrix $\Pi_{G}$ is the $\abs{Q}\times\abs{Q}$ matrix with the $(q, q')$-entry, written as $\pi(q, q')$, given by 
    \begin{gather}
       \pi(q, q') = \sum_{\mathclap{\sigma: \delta(q, \sigma)=q'}}\pitilde(q, \sigma).
   \end{gather}%
   \end{defn}%
   Both $\Pi_{G}$ and $\Pitilde_{G}$ are row-stochastic, \ie non-negative with rows of sum $1$. 
   Since the graph of a PFSA is strongly connected, there is a unique probability vector $\mathbf{p}_{G}$ that satisfies $\mathbf{p}_{G}^{T}\Pi_{G} = \mathbf{p}_{G}^{T}$, and is called the stationary distribution of $G$~\cite{vidyasagar2014hidden}.  
 %-----------
   \begin{defn}[$\Gamma$-Expression]
     \label{defn:GammaExpr}
    $\delta$ and $\pitilde$ may be encoded by a set of $|Q| \times |Q|$ matrices $\mathbf{\Gamma} = \set{\Gamma_\sigma| \sigma\in\Sigma}$, where
    \begin{gather}
       \Gamma_\sigma\big|_{q, q'} = \left\{
         \begin{array}{ll}
           \pitilde(q, \sigma) & \textrm{if } \delta(q, \sigma) = q',\\
           0 & \textrm{if otherwise}.
         \end{array}\right.
     \end{gather}%
     We extend the definition to $\Sigma^{\star}$ by $\Gamma_x=\prod^{n}_{i=1}\Gamma_{\sigma_i}$ for $x = \sigma_1\dots \sigma_n$ with $\Gamma_{\lambda} = I$, where $I$ is the identity matrix.
   \end{defn}
   \begin{defn}[Sequence-Induced Distributions]
     \label{defn:InducedDistr}
     For a PFSA $G = \paren{Q, \Sigma, \delta, \pitilde}$, the distribution on $Q$ induced by a sequence $x$ is given by $\mathbf{p}^{T}_{G}(x) = \nrm{\mathbf{p}_G^{T}\Gamma_{x}}$, where $\nrm{\mathbf{v}} = \mathbf{v}/\norm{\mathbf{v}}_1$. 
   \end{defn}
   \begin{defn}[Stochastic process generated by  PFSA]
     \label{defn:StochasticpocessOfPFSA}
     Let $G=\paren{Q, \Sigma, \delta, \pitilde}$ be a PFSA, the $\Sigma$-valued stochastic process $\set{X_t}_{t\in\Sigma}$ generated by $G$ satisfies that $X_1$ follows the distribution $\mathbf{p}_G^T\Pitilde_{G}$ and $X_{t+1}$ follows the distribution $\mathbf{p}_{G}\paren{X_{1}\cdots X_{t}}^T\Pitilde_{G}$ for $t\in\N$.
   \end{defn}
   % 
   We denote the probability an PFSA $G$ producing a sequence $x$ by $p_{G}(x)$. We can verify that $p_{G}(x) = \norm{\mathbf{p}_{G}^T\Gamma_x}_1$.


   \subsection{Process KL Divergence Measures}
   Detailed proofs of the propositions in this section are available in  cited work~\cite{chattopadhyay2020deep}. The notion of process KL divergence generalizes the analogous, well-studied notion for probability distributions~\cite{cover2012elements}.
 \label{sec:ProcessKLDiv}
   \begin{defn}[Entropy rate and Process KL divergence]\label{defKL}
     The entropy rate $\mathcal{H}(G)$ of a PFSA $G$ is the entropy rate of the  process generated by $G$~\cite{cover2012elements}. Similarly, the KL divergence $\DKL{G}{G'}$ of a PFSA $G'$ from the PFSA $G$ is the KL divergence of the process generated by the $G'$ from that of $G$~\cite{vidyasagar2007bounds}:
 %
    \cgather{
       \mathcal{H}(G) = -\lim_{d\rightarrow\infty}\frac{1}{d}\sum_{x\in\Sigma^{d}}p_{G}(x)\log p_{G}(x)\\
       \mathcal{D}_{\textrm{KL}}\parenBar{G}{G'} = \lim_{d\rightarrow\infty}\frac{1}{d} \sum_{x\in\Sigma^{d}}p_{G}(x)\log\frac{p_{G}(x)}{p_{G'}(x)},
     }%
     whenever the limits exist.
   \end{defn}
   %
   \begin{lem}
     For any PFSA $G,H$, KL divergence satisfies:
     \cgather{
       \DKL{G}{H} \geqq 0 \\
       \DKL{G}{H} = 0 \textrm{ iff  } G = H
     }
     where we interpret equality of PFSA $G,H$ as
     \cgather{\label{PFSAdistinct}
 \forall x \in \Sigma^\star, p_G(x) = p_H(x) \Rightarrow G=H
       }
   \end{lem}
   \begin{IEEEproof}
 Follows from the standard argument for  non-negativity of KL divergence for probability distributions~\cite{cover2012elements}.
   \end{IEEEproof}
   % ########################################
   % #############################################
   % ###########################################################
   % ###########################################################
  
   \begin{defn}[Log-likelihood]
     \label{defn:llk}
     The log-likelihood~\cite{cover2012elements} of a PFSA $G$ generating $x\in\Sigma^{d}$ is given by
\cgather{       L(x, G) = -\frac{1}{d}\log p_{G}(x).
     }%
   \end{defn}
   % ###########################################################
   % ###########################################################
   \begin{thm}[Convergence of Log-likelihood]
 	\label{thm:llkConvergence}
     Let $G$ and $H$ be two irreducible PFSA, and let $x\in\Sigma^d$ be a sequence generated by $G$. Then we have
     \cgather{
       L(x, H)\rightarrow \mathcal{H}(G) + \mathcal{D}_{\textrm{KL}}\parenBar{G}{H},
     }%
     in probability as $d\rightarrow\infty$.
   \end{thm}
   % ###########################################################
   % \begin{IEEEproof}
   %   By chain rule
   %   \begin{align*}
   %     &\sum_{x\in\Sigma^{d}}p_{G}(x)\log\frac{p_{G}(x)}{p_{H}(x)}\\
   %     =& \sum_{x\in\Sigma^{d-1}}\sum_{\sigma\in\Sigma}p_{G}(x)\mathbf{p}_{G}^{T}(x)\left.\Pitilde_{G}\right|_{\sigma}\log\frac{p_{G}(x)\mathbf{p}_{G}(x)^{T}\left.\Pitilde_{G}\right|_{\sigma}}{p_{H}(x)\mathbf{p}_{H}(x)^{T}\left.\Pitilde_{H}\right|_{\sigma}}\\
   %     =&\sum_{x\in\Sigma^{d-1}}p_{G}(x)\log\frac{p_{G}(x)}{p_{H}(x)} \notag\\
   %     &\mspace{20mu}+ \underbrace{\sum_{x\in\Sigma^{d-1}}p_{G}(x)\sum_{\sigma\in\Sigma}\mathbf{p}_{G}(x)^{T}\left.\Pitilde_{G}\right|_{\sigma} \log\frac{\mathbf{p}_{G}(x)^{T}\left.\Pitilde_{G}\right|_{\sigma}}{\mathbf{p}_{H}(x)^{T}\left.\Pitilde_{H}\right|_{\sigma}}}_{D_d}.
   %   \end{align*}
   %   By induction, we have $\mathcal{D}_{\textrm{KL}}\parenBar{G}{H} = \lim_{d\rightarrow\infty}\frac{1}{d}\sum_{i=1}^{d}D_{i}$, 
   %   and hence by Ces\`{a}ro summation theorem \cite{hardy1992divergent}, we have 
   %      \[
   %        \mathcal{D}_{\textrm{KL}}\parenBar{G}{H} = \lim_{d\rightarrow\infty} D_d.
   %   \]
 
   %   Let $x=\sigma_1\sigma_2...\sigma_n$ be a sequence generated by $G$ and $x^{[i-1]}$ be the truncation of $x$ at the $(i-1)$-th symbols, we have
   %   \begin{align*}
   %     &-\frac{1}{n}\sum_{i=1}^{n}\log\mathbf{p}_{H}\paren{x^{[i-1]}}^{T}\left.\Pitilde_{H}\right|_{\sigma_i}\\
   %     =& \underbrace{\frac{1}{n}\sum_{i=1}^{n}\log\frac{\mathbf{p}_{G}\paren{x^{[i-1]}}^{T}\left.\Pitilde_{G}\right|_{\sigma_{i}}}{\mathbf{p}_{H}\paren{x^{[i-1]}}^{T}\left.\Pitilde_{H}\right|_{\sigma_i}}}_{A_{x, n}}\notag\\
   %     &-\underbrace{\frac{1}{n}\sum_{i=1}^{n}\log\mathbf{p}_{G}\paren{x^{[i-1]}}^{T}\left.\Pitilde_{G}\right|_{\sigma_i}}_{B_{x, n}}.
   %   \end{align*}Because the process generated by $G$ is ergodic, we have
   %   \[
   %     \lim_{n\rightarrow\infty}A_{x,n} = \lim_{d\rightarrow\infty}D_d = \mathcal{D}_{\textrm{KL}}\parenBar{G}{H}.
   %   \]
   %   and $\lim_{n\rightarrow\infty}B_{x,n} = \mathcal{H}(G)$.
   % \end{IEEEproof}
    %########################################
   % ###########################################################

 Next, we denote the  log-likelihood of PFSA $H$ generating a sequence $x$ of length $d$  which is actually generated by PFSA $G$ as $L\paren{x\xleftarrow{d} G, H}$. We show that the probability  that sequences $x,y$ generated by distinct processes    cannot be distinguished by a random set of PFSA vanishes with enough data.
   %########################################
   %########################################
   \begin{thm}[Approximate Metric]\label{thmapproxmetric}
   	Let $X$ and $Y$ be two distinct PFSA in the sense of Eq.~\eqref{PFSAdistinct}, and $x,y$ be of length at least $d$ generated respectively by $X,Y$. If $\mathcal{G}$ is a randomly chosen set of $k$ PFSA, then
 $Pr(\theta_\mathcal{G}(x,y)=0) \rightarrow 0$, as $d,k \rightarrow \infty$.
   \end{thm}
   %########################################
   % \begin{IEEEproof}
   %   Because of Thm.~\ref{thm:llkConvergence}, 
   %   we start the proof by showing a fact about entropy and KL divergence: 
   %   Let 
   %   \[
   % 	  D_{X, Y}(G) = \abs{H(X) + \mathcal{D}_{\textrm{kl}}\parenBar{X}{G} - \paren{H(Y) + \mathcal{D}_{\textrm{kl}}\parenBar{Y}{G}}},
   %   \]
   %   then either $D_{X, Y}(X) > 0$ or $D_{X, Y}(Y) > 0$. 
   %   In fact, let us assume on the contrary that
   %   \begin{align*}
   % 	  D_{X, Y}(X) =& \abs{H(X) - \paren{H(Y) + \mathcal{D}_{\textrm{kl}}\parenBar{Y}{X}}} = 0,\\
   % 	  D_{X, Y}(Y) =& \abs{H(X) + \mathcal{D}_{\textrm{kl}}\parenBar{X}{Y} - \paren{H(Y)}} = 0.
   %   \end{align*}
   %   Since $X$ and $Y$ are not equivalent, we have 
   %   \begin{align*}
   % 	  H(X) - H(Y) =& \mathcal{D}_{\textrm{kl}}\parenBar{Y}{X} > 0,\textrm{ and}\\
   % 	  H(X) - H(Y) =& -\mathcal{D}_{\textrm{kl}}\parenBar{X}{Y} < 0,
   %   \end{align*} 
   %   which is a contradiction.
   
   %   Now, without loss of generality, let us assume that $D_{X, Y}(X) = c_{X, Y} > 0$ and let
   %   \[
   % 	  \mathcal{A}_{X, Y} = \set{G:\, D_{X, Y}(G) \geq c_{X, Y}/2}.
   %   \]
   %   Since $X\in\mathcal{A}_{X, Y}$, 
   %   by continuity of entropy and KL divergence,  
   %   we have $p_{X, Y} = Pr\paren{\mathcal{A}_{X, Y}} > 0$. 
   
   %      Again by Thm.~\ref{thm:llkConvergence}, we have
   %   \[
   % 	  \abs{L\paren{x\xleftarrow{d} X, G} - L\paren{y\xleftarrow{d} Y, G}} \rightarrow D_{X, Y}(G),
   %   \]  
   %   and hence, for $G\in\mathcal{A}_{X, Y}$,
   %   \[
   % 	  Pr\paren{L(x\xleftarrow{d} X, G) = L(y\xleftarrow{d} Y, G)} \rightarrow 0
   %   \]
   %   as $d\rightarrow\infty$. 
 
   %      Let $\mathcal{G}$ be a set of $k$ randomly chosen PFSA. 
   %   From the analysis above, we see that  
   %   as long as $\mathcal{G}\cap\mathcal{A}_{X, Y} \neq\emptyset$, 
   %   the probability that $\mathcal{G}$ cannot distinquish 
   %   sequences generate by $X$ and $Y$ vanishes 
   %   as sequence length approaches infinity.
   %   However, since a randomly chosen set $\mathcal{G}$ of $k$ PFSA satisfies 
   %   \[
   % 	  Pr\paren{\mathcal{G} \cap \mathcal{A}_{X, Y}\neq\emptyset} = 1 - (1 - p_{X, Y})^{k}\rightarrow 1
   %   \] 
   %   as $k\rightarrow\infty$, we conclude the proof.   
   % \end{IEEEproof}
   % #############################################
 
   % We illustrate the claim in Theorem~\ref{thm:llkConvergence} in  Fig.~\ref{fig:Projection} with a pair of randomly generated PFSA 
   % project sequences generated by two distinct PFSA. With two fixed PFSA $G_1$ and $G_2$ over the binary alphabet,  we generate from each model $100$ sequences, each  of length $100$. Then, we randomly choose two PFSAs $\mathcal{G}=\{T_1,T_2\}$, and compute  \ALGONAME using Eq.~\eqref{SLDeq}, showing that  the sequences  remain well-separated.
% 
\subsubsection{Implementation Issues \& Complexity}
   \label{sec:SLD}
 %###########################################
 %###########################################
   \begin{algorithm}[t]
     \SetAlgoLined
     \KwData{A PFSA $G = \paren{Q, \Sigma, \delta, \pitilde}$ and a sequence $x$ of length $n$.}
     \KwResult{Log-likelihood of $G$ generating $x$}
     Get the stationary distribution $\mathbf{p}_G$ as the left eigenvector of $\Pi_{G}$ of eigenvalue $1$\;
     Let $\mathbf{p}$ be the current distribution on states, and initialize it with $\mathbf{p}_{G}$\;
     Let $L$ be the log-likelihood of $G$ generating $x$ and initialize it with $0$\;
     \For{each symbol $\sigma$ in $x$}{
       Get the current distribution on symbols $\phi = \mathbf{p}_{G}^{T}\Pitilde_{G}$\;
       Update $L = L - \log\phi(\sigma)$\;
       Let $\mathbf{p}_{\textrm{new}}$ be the new distribution on states, and initialize all its entries with $0$\; 
       \For{each state $q\in Q$}{
         Let the next the state $q_{\textrm{new}} = \delta(q, \sigma)$\;
         Let $\mathbf{p}_{\textrm{new}}(q_{\textrm{new}}) = \mathbf{p}_{\textrm{new}}(q_{\textrm{new}}) + \mathbf{p}(q)\pitilde(q, \sigma)$\;
       }
       Update $\mathbf{p}$ with $\mathbf{p}_{\textrm{new}} /\norm{\mathbf{p}_{\textrm{new}}}_1$\; 
     }
     Let $L = L / n$\;
     \Return $L$\;
     \caption{PFSA Log-likelihood}
     \label{alg:PFSAllk}
   \end{algorithm}
 %###########################################
 %###########################################


   The algorithm for evaluating the log-likelihood of a PFSA generating a given sequence is given in Alg.~\ref{alg:PFSAllk}.    It is immediate that the time complexity of log-likelihood evaluation is  $O\paren{d\times|Q|}+A$ with $d$ is the input length   and $|Q|$ is the number of states in the PFSA being considered, and $A$ is the complexity of computing the stationary eigenvector in step 1.	We note that the complexity for likelihood scoring of HMMs with the forward algorithm  has time complexity $O\paren{d\times|Q|^2}$, where $Q$ is the number of the hidden states~\cite{rabiner1989tutorial}. Nothwithstanding asymptotic time complexities, Alg.~\ref{alg:PFSAllk} is clearly significantly simpler to the dynamic programmimg involved in the forward algorithm of HMM likelihood scoring.
 
 %##################################
 \subsubsection{\ALGONAME with fixed base sets}
As fixed base sets,,  we use $\mathcal{G}$ composed the four simple PFSA shown. While better results may be obtained by random set of base models, using a fixed set yields sufficiently good performance when compared with the state of art.  In contrast  to using a fixed set of base models, we can also infer good base models in a classification problem, by selecting as the base models the class-specific PFSA inferred from the training set. %This approach is further described  in Sec.~\ref{sec:AlgComp}.
   % ##########
   \subsubsection{\ALGONAME with continuous data}
   Since PFSA model sequences on finite alphabet, continuous-valued input should first be quantized to discrete ones. The simplest approach of discretization is to choose $k-1$ cut-off points $p_1 < p_2 < \cdots <p_{k-1}$ and replace a value $<p_1$ by $0$, in $[p_i, p_{i+1})$ by $i$, and $\geq p_{k-1}$ by $k$. We call the set of cut-off points a \emph{partition}. In our implementation, we use the  entropy maximization principle  to obtain bins in which data points are evenly distributed. If there are clear trends in the data stream, we carry out partitioning after detrending. Inference of the two base models is carried our  using the algorithm \textsf{GenESeSS}~\cite{chattopadhyay2013abductive}.

   \subsection{Inference of Probabilistic Automata}
   For PFSA inference we use  \textsf{GenESeSS}~\cite{chattopadhyay2013abductive}, outlined in Algorithm~\ref{alg:GenESeSS}.
   %#######################################################
\begin{algorithm}[!ht]
  \small
  \KwData{A sequence $x$ over alphabet $\Sigma$, $0< \varepsilon < 1$}
  \KwResult{State set $Q$, transition map $\delta$, and transition probability $\pitilde$}
  \tcc{\textcolor{PineGreen}{\textbf{Step One: Approximate $\varepsilon$-synchronizing sequence}}}
  Let $L=\ceil{\log_{\abs{\Sigma}}1/\varepsilon}$\; \label{algline:GenL}
  Calculate the \textbf{derivative heap} $\mathcal{D}^{x}_{\varepsilon}$ equaling $\set{\hat{\phi}^{x}_y\ :\ \textrm{$y$ is a sub-sequence of $x$ with } |y|\leq L}$\;
  Let $\mathcal{C}$ be the convex hull of $D^{x}_{\varepsilon}$\; \label{algline:GenConv}
  Select $x_0$ with $\hat{\phi}^{x}_{x_0}$ being a vertex of $\mathcal{C}$ and has the highest frequency in $x$\; \label{algline:GenSyncSeq}
  \tcc{\textcolor{PineGreen}{\textbf{Step Two: Identify transition structure}}}
  Initialize $Q = \set{q_0}$\; \label{algline:GenStep2Start}
  Associate to $q_0$ the \textbf{sequence identifier} $x^{\textrm{id}}_{q_0} = x_0$ and the probability vector $d_{q_0} = \hat{\phi}^x_{x_0}$\;
  Let $\widetilde{Q}$ be the set of states that are just added and initialize it to be $Q$\;
  \While{$\widetilde{Q}\neq\emptyset$}{\label{algline:GenExitCondition}
    Let $Q_\textrm{new} = \emptyset$ be the set of new states\;
    \For{$(q, \sigma)\in \widetilde{Q} \times \Sigma$}{
      Let $x = x^{\textrm{id}}_q$ and $d = \hat{\phi}^x_{x\sigma}$\;
      \eIf{$\norm{d-d_{q'}}_\infty < \varepsilon$ for some $q'\in Q$\label{algline:GenIdenStateStart}}{
        Let $\delta(q, \sigma) = q'$\;
      }{
        Let $Q_\textrm{new} = Q_\textrm{new}\cup\set{q_\textrm{new}}$ and $Q = Q\cup\set{q_\textrm{new}}$\;
        Associate to $q_\textrm{new}$ the sequence identifier $x_{q_\textrm{new}}^{\textrm{id}} = x\sigma$ and the probability vector $d_{q_\textrm{new}}=d$\;
        Let $\delta(q, \sigma) = q_\textrm{new}$\;
      }\label{algline:GenIdenStateEnd}
    }
    Let $\widetilde{Q} = Q_{\textrm{new}}$\;
  }
  Take a strongly connected subgraph of the labeled directed graph defined by $Q$ and $\delta$, and denote the vertex set of the subgraph again by $Q$\;\label{algline:GenStep2End}
  \tcc{\textcolor{PineGreen}{\textbf{Step Three: Identify transition probability}}}
  Initialize counter $N\bracket{q,\sigma}$ for each pair $(q, \sigma) \in Q\times\Sigma$\; \label{algline:GenIdenTransProbStart}
  Choose a random starting state $q\in Q$\;
  \For{$\sigma\in x$}{
    Let $N\bracket{q,\sigma} = N\bracket{q,\sigma} + 1$\;
    Let $q = \delta\paren{q,\sigma}$\;
  }
  Let $\pitilde\paren{q} = \nrm{\paren{N\bracket{q,\sigma}}_{\sigma\in\Sigma}}$\;\label{algline:GenIdenTransProbEnd}
  \Return $Q$, $\delta$, $\pitilde$\;
  \caption{\algo}
  \label{alg:GenESeSS}
\end{algorithm}
 %##################################
 %##################################
\section{Notion of Anomaly Discovery}
   
%#####################################################

% ######### END DISPLAYBREAKS
}
% ###########################



              

\bibliographystyle{siam}
\bibliography{bibliography}
\end{document}
